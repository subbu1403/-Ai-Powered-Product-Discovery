# -*- coding: utf-8 -*-
"""productGPT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19-cSdcApY2VdVUkAmtiguVcADqJtyWY5
"""
import streamlit as st
import pandas as pd
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config
from torch.utils.data import Dataset, DataLoader
from sentence_transformers import SentenceTransformer, util
from PIL import Image
import matplotlib.pyplot as plt

# Define paths
MODEL_NAME = "gpt2-medium"
TOKENIZER = GPT2Tokenizer.from_pretrained(MODEL_NAME)
CONFIG = GPT2Config.from_pretrained(MODEL_NAME)

# Ensure PAD token exists
TOKENIZER.pad_token = TOKENIZER.eos_token

# Load sentence transformer model for similarity comparison
SIMILARITY_MODEL = SentenceTransformer("all-MiniLM-L6-v2")


class ProductDescriptionDataset(Dataset):
    def __init__(self, csv_file, tokenizer, max_length=128):
        self.data = pd.read_csv(csv_file)
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        product_name = str(self.data.iloc[idx].get("product_name", "Unknown"))
        category = str(self.data.iloc[idx].get("category", "Unknown"))
        price = str(self.data.iloc[idx].get("discounted_price", "Unknown"))
        link = str(self.data.iloc[idx].get("product_link", "Unknown"))
        about_product_review = str(self.data.iloc[idx].get("review_content", "No Review"))

        input_text = f"{product_name}. Category: {category}. Price: {price}. Link: {link}. About Product Review: {about_product_review}"

        encoded = self.tokenizer.encode_plus(
            input_text,
            max_length=self.max_length,
            truncation=True,
            padding="max_length",
            return_tensors="pt"
        )

        return encoded["input_ids"].squeeze(0), encoded["attention_mask"].squeeze(0)


def recommend_products(user_description, dataset, similarity_model, top_n=5):
    """
    Recommend top N products based on the user's product description.
    """
    # Compute embedding for user input
    user_embedding = similarity_model.encode(user_description, convert_to_tensor=True)

    product_texts = []
    product_details = []

    # Extract product details for similarity matching
    for idx in range(len(dataset.data)):
        product_name = dataset.data.iloc[idx]["product_name"]
        category = dataset.data.iloc[idx]["category"]
        price = dataset.data.iloc[idx]["discounted_price"]
        link = dataset.data.iloc[idx]["product_link"]
        about_review = dataset.data.iloc[idx]["review_content"]

        product_text = f"{product_name}. Category: {category}. Price: {price}. About Review: {about_review}"
        product_texts.append(product_text)
        product_details.append((product_name, category, price, link))  # Storing details for results

    # Compute embeddings for all product descriptions
    product_embeddings = similarity_model.encode(product_texts, convert_to_tensor=True)

    # Compute cosine similarity
    similarities = util.pytorch_cos_sim(user_embedding, product_embeddings)[0]

    # Get top N recommendations
    top_indices = torch.argsort(similarities, descending=True)[:top_n]

    recommendations = []
    for idx in top_indices:
        product_name, category, price, link = product_details[idx]
        recommendations.append(
            {
                "Product Name": product_name,
                "Category": category,
                "Price": price,
                "Product Link": link,
                "Similarity Score": similarities[idx].item(),
            }
        )

    return recommendations

# Load dataset
csv_path = "amazon.csv"
train_dataset = ProductDescriptionDataset(csv_path, TOKENIZER)

# Load the YOLOv5 model (you can replace 'yolov5s' with your custom model)
model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)


def display_results(user_description):
    # Get recommended products
    recommended_products = recommend_products(user_description, train_dataset, SIMILARITY_MODEL)

    # # Display recommendations
    # print("\nTop Recommended Products:")
    # for idx, product in enumerate(recommended_products, 1):
    #     print(f"\n{idx}. {product['Product Name']}")  # Fixed key name
    #     print(f"   - Category: {product['Category']}")
    #     print(f"   - Price: {product['Price']}")
    #     print(f"   - Link: {product['Product Link']}")
    #     print(f"   - Similarity Score: {product['Similarity Score']:.4f}")
    st.write("### Top Recommended Products:")
    for idx, product in enumerate(recommended_products, 1):
        st.write(f"**{idx}. {product['Product Name']}**")  # Product Name in bold
        st.write(f"- **Category:** {product['Category']}")
        st.write(f"- **Price:** {product['Price']}")
        st.write(f"- [ðŸ”— Product Link]({product['Product Link']})")  # Clickable link
        st.write(f"- **Similarity Score:** `{product['Similarity Score']:.4f}`")  # Code block style
        st.write("---")  # Divider line


import torch
import cv2
import numpy as np
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration
from ultralytics import YOLO

# Load YOLOv8 model for object detection
detector = YOLO("yolov8n.pt")  # Using the smallest version for speed

# Load BLIP model for image captioning
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
caption_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

def detect_products(image):
    """Detect products in the image using YOLOv8."""
    results = detector(image)
    detected_objects = []

    for r in results:
        for box in r.boxes:
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            detected_objects.append((x1, y1, x2, y2))

    return detected_objects

def generate_caption(image):
    """Generate a product-focused description using BLIP."""
    inputs = processor(images=image, return_tensors="pt")
    output = caption_model.generate(**inputs)
    caption = processor.decode(output[0], skip_special_tokens=True)
    return caption

def process_image(image_path, output_path="output.jpg"):
    """Process an image: detect products, draw bounding boxes, and generate captions."""
    image = Image.open(image_path).convert("RGB")
    image_np = np.array(image)

    # Detect products
    boxes = detect_products(image_np)

    # Draw bounding boxes
    for x1, y1, x2, y2 in boxes:
        cv2.rectangle(image_np, (x1, y1), (x2, y2), (0, 255, 0), 2)

    # Save processed image
    output_image = Image.fromarray(image_np)
    output_image.save(output_path)

    # Generate and return product captions
    captions = []
    for i, (x1, y1, x2, y2) in enumerate(boxes):
        cropped_img = image.crop((x1, y1, x2, y2))
        caption = generate_caption(cropped_img)
        captions.append(f"Product {i+1}: {caption}")

    return captions
# Streamlit UI
st.title("Product Description or Image Input")

# User choice
user_input_type = st.radio("Choose Input Method:", ["Product Description", "Image Upload"])

if user_input_type == "Product Description":
    user_description = st.text_area("Enter Product Description:")
    if st.button("Generate Results"):
        display_results(user_description)
else:
    uploaded_file = st.file_uploader("Upload an Image", type=["jpg", "jpeg", "png"])
    if uploaded_file is not None:
        image = Image.open(uploaded_file)
        st.image(image, caption="Uploaded Image", use_column_width=True)
        captions = process_image(uploaded_file)
        for caption in captions:
            display_results(caption)

